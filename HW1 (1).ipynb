{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6cc1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd8e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation fumction ReLU and its derivative\n",
    "def ReLu_d(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "def ReLu(x):\n",
    "    return np.where(x < 0, 0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8b5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor->（multidimensional array）\n",
    "\n",
    "#d input nodes k hidden layer\n",
    "K = 10\n",
    "d = 10\n",
    "batch_size = 1\n",
    "\n",
    "X = Variable(torch.rand(batch_size,d),requires_grad=True)\n",
    "y = torch.tensor([[torch.sum(X**2)/d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3913003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        global z,z_relu\n",
    "        z = []\n",
    "        z_relu = []\n",
    "        super(NN1, self).__init__()\n",
    "        self.layer = nn.Sequential()\n",
    "        self.layer.add_module(\"h_1\", nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        z.append(self.layer(X))\n",
    "        z_relu.append(self.layer(X))\n",
    "        \n",
    "        for i in range(K-1):\n",
    "             index = str(i+2)\n",
    "             self.layer.add_module(\"h_\"+index, nn.Linear(hidden_size, hidden_size))\n",
    "             z.append(self.layer(X)) \n",
    "             self.layer.add_module(\"h_\"+index+\"_relu\"+index,nn.ReLU())\n",
    "             z_relu.append(self.layer(X))\n",
    "                \n",
    "        self.layer.add_module(\"h_last\", nn.Linear(hidden_size, num_classes))\n",
    "        z.append(self.layer(X))\n",
    "        #self.layer.add_module(\"h_1ast_relu\"+index,nn.ReLU())\n",
    "        z_relu.append(self.layer(X))\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.layer(X)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57be7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN1(d,2*d+1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc0ee92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08377084136009216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(X)\n",
    "loss = (y_pred - y)**2\n",
    "loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e13c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0924]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b3ba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 21]             231\n",
      "            Linear-2                [-1, 1, 21]             462\n",
      "              ReLU-3                [-1, 1, 21]               0\n",
      "            Linear-4                [-1, 1, 21]             462\n",
      "              ReLU-5                [-1, 1, 21]               0\n",
      "            Linear-6                [-1, 1, 21]             462\n",
      "              ReLU-7                [-1, 1, 21]               0\n",
      "            Linear-8                [-1, 1, 21]             462\n",
      "              ReLU-9                [-1, 1, 21]               0\n",
      "           Linear-10                [-1, 1, 21]             462\n",
      "             ReLU-11                [-1, 1, 21]               0\n",
      "           Linear-12                [-1, 1, 21]             462\n",
      "             ReLU-13                [-1, 1, 21]               0\n",
      "           Linear-14                [-1, 1, 21]             462\n",
      "             ReLU-15                [-1, 1, 21]               0\n",
      "           Linear-16                [-1, 1, 21]             462\n",
      "             ReLU-17                [-1, 1, 21]               0\n",
      "           Linear-18                [-1, 1, 21]             462\n",
      "             ReLU-19                [-1, 1, 21]               0\n",
      "           Linear-20                 [-1, 1, 1]              22\n",
      "================================================================\n",
      "Total params: 4,411\n",
      "Trainable params: 4,411\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7df8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d5da1",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in model.named_parameters():\n",
    "    #print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "277028c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save auto w,b\n",
    "auto_w =[]\n",
    "auto_b =[]\n",
    "i = 0\n",
    "\n",
    "for param in model.parameters():\n",
    "    if (i%2==0):\n",
    "        #auto_w.append(param.grad)\n",
    "        auto_w.append(np.round(np.array(param.grad).astype(np.double),5))\n",
    "    if (i%2==1):\n",
    "        #auto_b.append(param.grad)\n",
    "         auto_b.append(np.round(np.array(param.grad).astype(np.double),5))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc3605",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47732939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#手动grad share the same weight and bias with auto_forward\n",
    "parameters = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb9cf4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual forward\n",
    "def feedforward(X,parameters):\n",
    "    global z_manual,z_relu_manual\n",
    "    z_manual=[]\n",
    "    z_relu_manual=[]\n",
    "    z_manual.append(torch.from_numpy(np.dot(X.detach().numpy(),(parameters[0].detach().numpy().T))+parameters[1].detach().numpy()))\n",
    "    z_relu_manual.append(z_manual[0])\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        z_manual.append(torch.from_numpy(np.dot(z_relu_manual[i],(parameters[(i+1)*2].detach().numpy().T))+parameters[(i+1)*2+1].detach().numpy()))\n",
    "        z_relu_manual.append(ReLu(z_manual[i+1]))\n",
    "    z_relu_manual[-1]=z_manual[-1]\n",
    "    return z_relu_manual[-1]\n",
    "y_pred = feedforward(X,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "466fc8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_z=0\n",
    "for i in range(len(z)):\n",
    "    diff_z = diff_z+ np.absolute(sum(sum(z[i].detach().numpy()-z_manual[i].detach().numpy())))\n",
    "diff_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fd27986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_z_relu=0\n",
    "for i in range(len(z_relu)):\n",
    "    diff_z_relu = diff_z_relu+ np.absolute(sum(sum(z_relu[i].detach().numpy()-np.array(z_relu_manual[i]))))\n",
    "diff_z_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc2d5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#求对z的的倒数\n",
    "# dL/dv5\n",
    "d_z = []\n",
    "d_w = []\n",
    "torch.set_printoptions(precision=5)\n",
    "grad_y_pred = 2.0 * (y_pred - y) #[1,1]\n",
    "grad_z_y = grad_y_pred.detach().numpy()#*ReLu_d(y_pred)#dL/dz5,5)\n",
    "grad_w_y = grad_z_y*z_relu_manual[-2]#.detach().numpy()\n",
    "d_z.append(grad_z_y)\n",
    "d_w.append(grad_w_y)\n",
    "#from the last hidden layer\n",
    "for i in range (9):#i=1,2,3,4,5.... p:-4,-6,-8\n",
    "    grad_z_i = np.dot(d_z[i],ReLu_d(z_manual[-i-2])*parameters[-(i+1)*2].detach().numpy())\n",
    "    d_z.append(grad_z_i)\n",
    "    grad_w_i = np.dot(grad_z_i.T,z_relu_manual[-3-i])#.detach().numpy() )#-4,-5,-6\n",
    "    d_w.append(grad_w_i)\n",
    "    \n",
    "#first layer\n",
    "grad_z_1 = np.dot(d_z[9],ReLu_d(z_manual[-11])*parameters[-20].detach().numpy())\n",
    "d_z.append(grad_z_1)\n",
    "grad_w_1 = np.dot(grad_z_1.T,X.detach().numpy())\n",
    "d_w.append(grad_w_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2da25",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31c17c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(d_w)):\n",
    "    d_w[i]= np.round(np.array(d_w[i]).astype(np.double),5)\n",
    "    d_z[i]= np.round(np.array(d_z[i]).astype(np.double),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a043924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_w.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c24baa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "531a99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chaeck value difference between autograd and mygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75c07cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_w=0\n",
    "for i in range(len(d_w)):\n",
    "    diff_w = diff_w+ np.absolute(sum(sum(d_w[i]-auto_w[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1812820b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0000000000000012e-05"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08f7c53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3881317890172014e-21"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_b=0\n",
    "for i in range(len(d_z)):\n",
    "    diff_b = diff_b+ np.absolute(sum(sum(d_z[i]-auto_b[i])))\n",
    "diff_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "24eae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('torch_autograd.dat', 'w') as f:\n",
    "    f.write('autograd_w' + '\\n')\n",
    "    i = 1\n",
    "    for w in auto_w:\n",
    "        f.write('weight_' +str(i)+ '\\n')\n",
    "        f.write(str(w)+'\\n')\n",
    "        i+=1\n",
    "    f.write('autograd_b' + '\\n')\n",
    "    i = 1\n",
    "    for b in auto_b:\n",
    "        f.write('bias_' +str(i)+ '\\n')\n",
    "        f.write(str(b)+'\\r\\n')\n",
    "        i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6ef02bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_autograd.dat', 'w') as f:\n",
    "    f.write('my_w' + '\\n')\n",
    "    i = 1\n",
    "    for w in d_w:\n",
    "        f.write('weight_' +str(i)+ '\\n')\n",
    "        f.write(str(w)+'\\n')\n",
    "        i+=1\n",
    "    f.write('my_b' + '\\n')\n",
    "    i = 1\n",
    "    for b in d_z:\n",
    "        f.write('bias_' +str(i)+ '\\n')\n",
    "        f.write(str(b)+'\\r\\n')\n",
    "        i+=1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
